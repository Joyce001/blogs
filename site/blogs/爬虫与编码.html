
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
        <meta name="uyan_auth" content="711800a0ee">
        <link rel="stylesheet" type="text/css" href="/blogs/site/css/main.css">
        <script src="/blogs/site/js/jquery.js"></script>
        
        <script type="text/javascript">
            var _hmt = _hmt || [];
            (function() {
              var hm = document.createElement("script");
              hm.src = "//hm.baidu.com/hm.js?975a5aad9775f4e01af87fb7c67b8f63";
              var s = document.getElementsByTagName("script")[0];
              s.parentNode.insertBefore(hm, s);
            })();
        </script>
        <script>
        var CONFIG = {
            demoDir: '/blogs',
            demoexpstr: 'Chrome'
        };
        </script>
        
        
        <title>爬虫与编码</title>
    
    </head>
    <body>
        <div class="navbar navbar-dark bg-inverse">
            <div class="container">
                <a href="/blogs/site/index.html" class="navbar-brand">首页</a>
            </div>
        </div>
        
        <div class="blog-page container">
            <div class="card">
                <div class="card-block">
                    <div class="hljs">
                        <!-- config.time: 2016-01-09 13:23 -->
<!-- config.brief: 作为一名 web 开发人员，时不时爬爬别人家的网站还是很有趣的。其实，爬一个网站的数据也是爬取者和被爬取者的一种攻防较量：一般情况下，被爬取者总是会想方设法地阻止爬取者爬取自家网站数据。 -->
<h1 id="-">爬虫与编码</h1>
<p>作为一名 web 开发人员，时不时爬爬别人家的网站还是很有趣的。</p>
<p>其实，爬一个网站的数据也是爬取者和被爬取者的一种攻防较量：一般情况下，被爬取者总是会想方设法地阻止爬取者爬取自家网站数据。</p>
<p>于是，一些网站就会采取一些措施来阻止非正常访问：</p>
<ul>
<li>某某关键接口只能每分钟调用若干次；</li>
<li>某某 IP 访问网站太频繁，直接拒绝掉该 IP 发过来的请求。</li>
</ul>
<p>当然，这些措施都只是治标不治本，不能从根本上杜绝自己网站数据被爬。</p>
<p>从爬取者的角度来看，要突破层层限制，拿到目标网站的数据，还是要做一些事情的：</p>
<ul>
<li>如果要爬取的目标网页需要登录才能访问到，那么可以采用 phantomjs 来简化掉 session 的处理；</li>
<li>在爬取的过程中，如果发现服务器从某个时刻开始一直拒绝掉自己的请求，那么就要怀疑自己的 IP 是否被屏蔽掉了，或者某个接口是否访问太频繁了；</li>
<li>对于有 IP 限制策略的网站，尽量模拟正常用户访问，频率不要太快，最好做多个节点来爬取。</li>
</ul>
<p><strong>等等，有点偏题了！下面进入正轨：</strong></p>
<p>在初次写爬虫代码的时候，很容易遇到解析出来的数据是乱码的问题：</p>
<p><img src="https://github.com/yibuyisheng/blogs/blob/master/imgs/12.png?raw=true" alt=""></p>
<p>面对这些乱码，如何解决呢？</p>
<h3 id="-http-">注意 HTTP 响应的头部</h3>
<p>留意一下 HTTP 响应的头部是否有用 gzip 压缩过：</p>
<pre><code><span class="hljs-tag">Content-Encoding</span><span class="hljs-pseudo">:gzip</span>
</code></pre><p>如果有这种字眼，那么响应正文部分就是使用 gzip 压缩过的，在拿到这种压缩过的数据之后，要先解压。</p>
<p>Node.js 中，提供了 zlib 模块，用于处理 gzip 相关的操作。对于解压 gzip ，可以这样做：</p>
<pre><code class="lang-js"><span class="hljs-comment">// `res` 是响应对象，http.IncommingMessage 类型的</span>

<span class="hljs-keyword">var</span> buffers = [];<span class="hljs-comment">// 暂存 gzip 解压过后的 buffer</span>
<span class="hljs-keyword">var</span> size = <span class="hljs-number">0</span>;<span class="hljs-comment">// 记录整个响应体解压后的数据大小</span>
<span class="hljs-keyword">var</span> gunzipStream = zlib.createGunzip();
res.pipe(gunzipStream);
gunzipStream.on(<span class="hljs-string">'data'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">(buffer)</span> </span>{
    buffers.push(buffer);
    size += buffer.length;
});
gunzipStream.on(<span class="hljs-string">'error'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">(error)</span> </span>{
    <span class="hljs-comment">// 发生了错误，处理下吧！</span>
});
gunzipStream.on(<span class="hljs-string">'end'</span>, <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-params">()</span> </span>{
    <span class="hljs-keyword">var</span> unzipedBuffer = Buffer.concat(buffers, size); <span class="hljs-comment">// unzipedBuffer 就是解压过后的数据</span>
});
</code></pre>
<h3 id="-">注意文本编码</h3>
<p>拿着最终 gzip 解压出来的数据，开心的去进行后续处理，结果继续乱码，为什么会这样？</p>
<p>Node.js 里面默认字符串是 <code>utf-8</code> 编码的，如果 gzip 解压出来的数据不是 <code>utf-8</code> 编码的话，那么把这堆 buffer 数据转换成字符串的时候就可能产生乱码。 到目前为止，Node.js 内置支持的解码方式很有限，只能依靠一些第三方模块进行某些文本解码。</p>
<p>怎么办呢？</p>
<p>留意一下响应头当中的 <code>Content-Type</code> 部分，如果 charset 是非 <code>utf-8</code> 的话，那就要考虑继续对数据进行解码了。</p>
<p>对于中文网站，可能会使用 <code>GBK</code> 或者 <code>GB2312</code> 进行编码，对于此种场景，需要用到第三方的解码工具，此处选用了 <code>iconv-lite</code> 。解码过程如下：</p>
<pre><code class="lang-js"><span class="hljs-keyword">var</span> finallyResponseText = <span class="hljs-built_in">require</span>(<span class="hljs-string">'iconv-lite'</span>).decode(unzipedBuffer, <span class="hljs-string">'gbk'</span>);
</code></pre>
<p>这样，就拿到了最终想要的文本数据。</p>

                    </div>
                </div>
            </div>

            
                <div class="card">
                    <div class="card-block">
                        <div class="ds-thread"
                            data-thread-key="爬虫与编码"
                            data-title="爬虫与编码"
                            data-url="http://yibuyisheng.github.io/blogs/site/blogs/爬虫与编码.html">
                        </div>
                    </div>
                </div>
            
        </div>
        
            <script type="text/javascript">
                var duoshuoQuery = {short_name:"yibuyishengblogs"};
                (function() {
                    var ds = document.createElement('script');
                    ds.type = 'text/javascript';ds.async = true;
                    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
                    ds.charset = 'UTF-8';
                    (document.getElementsByTagName('head')[0]
                     || document.getElementsByTagName('body')[0]).appendChild(ds);
                })();
            </script>
        
        <script type="text/javascript" src="/blogs/site/js/blog.js"></script>
    
    </body>

